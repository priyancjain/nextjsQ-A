Hereâ€™s a polished **`README.md`** in proper markdown format for your project:

```markdown
# ğŸ“„ Mini PDF QA App

A minimal **Next.js** application that allows users to upload PDFs, generate embeddings, and query them in natural language using an LLM.  

---

## ğŸš€ Features
- **PDF Upload & Parsing** â€“ Extracts text from uploaded PDFs.  
- **Embeddings** â€“ Uses vector embeddings to store and search PDF content.  
- **Question Answering** â€“ Ask natural language questions about the uploaded PDFs.  
- **Chat UI** â€“ Interactive chat interface with message bubbles.  

---

## ğŸ“‚ Project Structure
```

app/
â”œâ”€â”€ api/ask/route.ts      # API route for answering questions
â”œâ”€â”€ api/upload/route.ts   # API route for uploading PDFs
â”œâ”€â”€ layout.tsx            # Root layout
â”œâ”€â”€ page.tsx              # Main entry page

components/
â”œâ”€â”€ FileUploader.tsx      # Upload UI component
â”œâ”€â”€ MessageBubble.tsx     # Chat bubble component
â”œâ”€â”€ QuestionBox.tsx       # Input box for asking questions

lib/
â”œâ”€â”€ embeddings.ts         # Embedding generation logic
â”œâ”€â”€ pdf.ts                # PDF parsing helpers
â”œâ”€â”€ vector.ts             # Vector database operations

````

---

## âš™ï¸ Setup Instructions

1. **Clone the repo**
   ```bash
   git clone https://github.com/your-username/mini-pdf-qa-app.git
   cd mini-pdf-qa-app
````

2. **Install dependencies**

   ```bash
   npm install
   ```

3. **Configure environment variables**
   Create a `.env.local` file in the root with your API key:

   ```env
   OPENAI_API_KEY=your_openai_api_key
   ```

4. **Run the development server**

   ```bash
   npm run dev
   ```

5. Open [http://localhost:3000](http://localhost:3000) in your browser ğŸ‰

---

## ğŸ’¡ Approach

1. **Upload & Extract** â€“ Users upload PDFs via `FileUploader.tsx`, handled by `app/api/upload/route.ts`. Text is extracted using `lib/pdf.ts`.
2. **Embeddings & Storage** â€“ Extracted text is converted into embeddings (`lib/embeddings.ts`) and stored in a vector store (`lib/vector.ts`).
3. **Querying** â€“ When users ask questions (`QuestionBox.tsx`), the backend (`app/api/ask/route.ts`) retrieves relevant chunks from the vector DB and queries the LLM for an answer.
4. **Interactive UI** â€“ Messages are displayed in a conversational format with `MessageBubble.tsx`.

---

## ğŸ“Œ Tech Stack

* **Next.js 13+ (App Router)**
* **TypeScript**
* **TailwindCSS**
* **OpenAI API**
* **Vector Search for Retrieval**

---

## ğŸ“œ License

This project is licensed under the MIT License.

```

---

Do you want me to also add a **deployment guide (Vercel)** section to this README so it's production-ready, or keep it minimal for local setup only?
```

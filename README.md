
````markdown
# ğŸ“„ Mini PDF QA App

A minimal **Next.js** application that allows users to upload PDFs, generate embeddings, and query them in natural language using an LLM.  

---

## ğŸš€ Features
- ğŸ“‘ **PDF Upload & Parsing** â€“ Extracts text from uploaded PDFs.  
- ğŸ§  **Embeddings** â€“ Uses vector embeddings to store and search PDF content.  
- â“ **Question Answering** â€“ Ask natural language questions about the uploaded PDFs.  
- ğŸ’¬ **Chat UI** â€“ Interactive chat interface with message bubbles.  

---

## ğŸ“‚ Project Structure
```bash
app/
â”œâ”€â”€ api/ask/route.ts      # API route for answering questions
â”œâ”€â”€ api/upload/route.ts   # API route for uploading PDFs
â”œâ”€â”€ layout.tsx            # Root layout
â”œâ”€â”€ page.tsx              # Main entry page

components/
â”œâ”€â”€ FileUploader.tsx      # Upload UI component
â”œâ”€â”€ MessageBubble.tsx     # Chat bubble component
â”œâ”€â”€ QuestionBox.tsx       # Input box for asking questions

lib/
â”œâ”€â”€ embeddings.ts         # Embedding generation logic
â”œâ”€â”€ pdf.ts                # PDF parsing helpers
â”œâ”€â”€ vector.ts             # Vector database operations
````

---

## âš™ï¸ Setup Instructions

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/mini-pdf-qa-app.git
   cd mini-pdf-qa-app
   ```

2. **Install dependencies**

   ```bash
   npm install
   ```

3. **Set up environment variables**
   Create a `.env.local` file in the root and add your OpenAI API key:

   ```env
   OPENAI_API_KEY=your_openai_api_key
   ```

4. **Run the development server**

   ```bash
   npm run dev
   ```

5. Open [http://localhost:3000](http://localhost:3000) in your browser ğŸ‰

---

## ğŸ’¡ Approach

1. **Upload & Extract** â€“ Users upload PDFs via `FileUploader.tsx`, handled by `app/api/upload/route.ts`. The text is extracted using `lib/pdf.ts`.
2. **Embeddings & Storage** â€“ Extracted text is converted into embeddings (`lib/embeddings.ts`) and stored in a vector store (`lib/vector.ts`).
3. **Querying** â€“ When users ask questions (`QuestionBox.tsx`), the backend (`app/api/ask/route.ts`) retrieves relevant chunks from the vector DB and queries the LLM for an answer.
4. **Interactive UI** â€“ Messages are displayed in a conversational format with `MessageBubble.tsx`.

---

## ğŸ“Œ Tech Stack

* âš›ï¸ **Next.js 13+ (App Router)**
* ğŸ”· **TypeScript**
* ğŸ¨ **TailwindCSS**
* ğŸ¤– **OpenAI API**
* ğŸ“š **Vector Search (for retrieval)**

---

## ğŸš€ Deployment (Vercel)

1. Push your project to GitHub.
2. Go to [Vercel](https://vercel.com/) and create a new project.
3. Import your GitHub repo.
4. Add your environment variable in **Vercel â†’ Project Settings â†’ Environment Variables**:

   ```
   OPENAI_API_KEY=your_openai_api_key
   ```
5. Deploy ğŸš€ â€” Vercel will build and host your app automatically.

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

```

---

Would you like me to also add a **Demo / Screenshots** section with placeholders (so you can drop in images of your UI later), or keep it clean and minimal?
```
